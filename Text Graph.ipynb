{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                 <font color=Black>TRIP ADVISER Review Analysis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As part of this analysis, we analyze the Hotel reviews from **Tripadviser**. we could find the [Trip Adviser dataset here](https://github.com/kavgan/OpinRank). This analysis is focused to **Text Graph** technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Packages\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import lil_matrix, spmatrix, csr_matrix, save_npz, load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "\n",
    "import datetime\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting cities and data path\n",
    "DataPath=\"E:\\\\Techi\\\\Courses\\\\Masters in Data Science\\\\Courses\\\\Text Visualization\\\\Data sets\\\\Trip Advisor\\\\hotels\\\\data\\\\\"\n",
    "Cities=['new-delhi','beijing', 'chicago', 'dubai', 'las-vegas', 'london', 'montreal',  'new-york-city', 'san-francisco', 'shanghai']\n",
    "Stopwords=[\"nan\",\"quot\",\"amp\",\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "Custom_Stopwords=[\"nan\",\"quot\",\"hotel\",\"room\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrivel and Processing\n",
    "\n",
    "Below code collect reviews from all files and append it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started : new-delhi\n",
      "Completed : new-delhi\n",
      "Started : beijing\n",
      "Completed : beijing\n",
      "Started : chicago\n",
      "chicago\\usa_illinois_chicago_the_whitehall_hotel file is corrupted\n",
      "Completed : chicago\n",
      "Started : dubai\n",
      "dubai\\are_dubai_towers_rotana_dubai file is corrupted\n",
      "Completed : dubai\n",
      "Started : las-vegas\n",
      "Completed : las-vegas\n",
      "Started : london\n",
      "london\\uk_england_london_best_western_phoenix_hotel file is corrupted\n",
      "london\\uk_england_london_hilton_london_green_park file is corrupted\n",
      "london\\uk_england_london_merlyn_court_hotel file is corrupted\n",
      "london\\uk_england_london_st_david_s_hotels file is corrupted\n",
      "Completed : london\n",
      "Started : montreal\n",
      "Completed : montreal\n",
      "Started : new-york-city\n",
      "new-york-city\\usa_new york city_park_central file is corrupted\n",
      "Completed : new-york-city\n",
      "Started : san-francisco\n",
      "Completed : san-francisco\n",
      "Started : shanghai\n",
      "Completed : shanghai\n"
     ]
    }
   ],
   "source": [
    "Dataset=pd.read_csv(DataPath+\"text.txt\", names=['Date','Title','Content'], delimiter=\"\\t\", index_col=3, encoding = \"ISO-8859-1\")\n",
    "Dataset[\"City\"]=\"Test\"\n",
    "Dataset.reset_index()\n",
    "for city in Cities :\n",
    "    print(\"Started : \"+city)\n",
    "    for i in os.listdir(DataPath+city+\"\\\\\"):\n",
    "        try :\n",
    "            Dataset2=pd.read_csv(DataPath+city+\"\\\\\"+i, names=['Date','Title','Content'], delimiter=\"\\t\", index_col=3, encoding = \"ISO-8859-1\")\n",
    "            Dataset2.reset_index()\n",
    "            Dataset2[\"City\"]=city\n",
    "            Dataset2[\"Hotel\"]=i\n",
    "            Dataset=pd.concat([Dataset,Dataset2],ignore_index=True,axis=0, sort=True)\n",
    "        except :\n",
    "            print(city+\"\\\\\"+i+\" file is corrupted\")\n",
    "    print(\"Completed : \"+city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset[\"Date\"]=pd.to_datetime(Dataset[\"Date\"])\n",
    "Dataset.index=Dataset.Date\n",
    "Dataset[\"Year\"]=Dataset.index.year\n",
    "Dataset[\"Month\"]=Dataset.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting hotel ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_review=pd.read_csv(DataPath+\"city_text.txt\", header=0 , delimiter=\"\\t\",encoding = \"ISO-8859-1\")\n",
    "for city in Cities :\n",
    "   \n",
    "    try :\n",
    "        city_review2=pd.read_csv(DataPath+city+\".csv\",header=0 , delimiter=\",\",  encoding = \"ISO-8859-1\",index_col=False)\n",
    "        city_review=pd.concat([city_review,city_review2],ignore_index=True,axis=0, sort=True)\n",
    "    except :\n",
    "        print(city+\"\\\\\"+i+\" file is corrupted\")\n",
    "city_review['overall_ratingsource']=city_review['overall_ratingsource'].replace(-1,np.NaN)\n",
    "#city_review[['doc_id','country','overall_ratingsource']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Gensim preparators\n",
    "RE_PUNCT = re.compile(r'([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
    "RE_NUMERIC = re.compile(r\"[0-9]+\", re.UNICODE)\n",
    "RE_NONALPHA = re.compile(r\"\\W\", re.UNICODE)\n",
    "RE_WHITESPACE = re.compile(r\"(\\s)+\", re.UNICODE)\n",
    "MIN_LENGTH = 3\n",
    "\n",
    "def normalise(s):\n",
    "    if s is None or s is np.nan:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = RE_PUNCT.sub(\" \", s)\n",
    "    s = RE_WHITESPACE.sub(\" \", s)\n",
    "    s = RE_NUMERIC.sub(\"\", s)\n",
    "    s = ' '.join([w for w in s.split() if len(w)>=MIN_LENGTH])\n",
    "    s = RE_NONALPHA.sub(\" \", s)\n",
    "    return s\n",
    "    \n",
    "#1 for skip-gram; otherwise CBOW.\n",
    "def create_model(Sentences2, Model_Name, sg1=0):\n",
    "    \n",
    "    print('Creating the model is starting : '+str(datetime.datetime.now()))\n",
    "    model_review = Word2Vec(sentences=list(pd.Series(Sentences2).dropna()) , size=100, window=5, min_count=1, workers=4, sg=sg1)\n",
    "    print('Creating the model is completed : '+str(datetime.datetime.now()))\n",
    "    if sg1==0 :\n",
    "        model_review.save(\"model\\\\\"+Model_Name+\"_\"+\"word2vec.model\")\n",
    "    else :\n",
    "        model_review.save(\"model\\\\\"+Model_Name+\"_\"+\"word2vec_Skipgram.model\")\n",
    "    print('Model is Saved : '+str(datetime.datetime.now()))\n",
    "    return model_review\n",
    "\n",
    "def load_model_file(modelfilename, list_of_words, sg1=0):\n",
    "    if sg1==0:\n",
    "        model= Word2Vec.load(\"model\\\\\"+modelfilename+\"_\"+\"word2vec.model\")\n",
    "    else :    \n",
    "        model= Word2Vec.load(\"model\\\\\"+modelfilename+\"_\"+\"word2vec_Skipgram.model\")\n",
    "    word1=[]\n",
    "    year=[]\n",
    "    s_word=[]\n",
    "    s_word_rate=[]\n",
    "    for x in list_of_words:\n",
    "        try :\n",
    "            for sw in model.wv.similar_by_word(x) :\n",
    "                word1.append(x)\n",
    "                year.append(np.nan)\n",
    "                s_word.append(sw[0])\n",
    "                s_word_rate.append(sw[1])\n",
    "        except :\n",
    "            pass\n",
    "    similiar_item=pd.DataFrame({\"word\": word1, \"s_word\":s_word, \"s_word_rate\":s_word_rate}) \n",
    "    if sg1==0:\n",
    "        similiar_item[\"Type\"]=modelfilename\n",
    "    else :\n",
    "        similiar_item[\"Type\"]=modelfilename+\"_sg\"\n",
    "    return similiar_item\n",
    "\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(sno.stem(token))\n",
    "    return result\n",
    "\n",
    "def create_sentence(rule):\n",
    "    print('Reading the sentence Started : '+str(datetime.datetime.now()))\n",
    "#    print(len(Dataset[\"Content\"][rule]))\n",
    "    Sentences=[]\n",
    "    for review in Dataset[\"Content\"][rule]:\n",
    "        if pd.isnull(review)== False and review!='':\n",
    "            tokens = preprocess(review)\n",
    "            tokens=[x for x in tokens if x not in Custom_Stopwords]\n",
    "            Sentences.append(tokens)\n",
    "            \n",
    "            #print(tokens)\n",
    "    print('Reading the sentence Completed : '+str(datetime.datetime.now()))\n",
    "    print('Total Sentences '+str(len(Sentences)))\n",
    "    return Sentences\n",
    "\n",
    "def corpora_dict_model(sentence1):\n",
    "    dictionary = corpora.Dictionary(sentence1)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentence1]\n",
    "    print('Creating the model is starting : '+str(datetime.datetime.now()))\n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, #passes=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    print('Creating the model is completed : '+str(datetime.datetime.now()))\n",
    "    return (doc_term_matrix,ldamodel)\n",
    "\n",
    "def samplesenteneces(Sentences1, limit=1000):\n",
    "    if (len(Sentences1) >= limit) :\n",
    "        print('Before restricting (size) : '+str(len(Sentences1)))\n",
    "        Sentences1=list(pd.Series(Sentences1).dropna()[np.linspace(0,len(Sentences1),limit,dtype='int')])\n",
    "        print('After restricting (size) : '+str(len(Sentences1)))\n",
    "        return list(pd.Series(Sentences1).dropna())\n",
    "    else :\n",
    "        print('Sentence size already with in a limit (size) : '+str(len(Sentences1)))\n",
    "        return list(pd.Series(Sentences1).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the sentence Started : 2019-05-18 18:50:16.864354\n",
      "Reading the sentence Completed : 2019-05-18 18:50:27.270950\n",
      "Total Sentences 3939\n",
      "Creating the model is starting : 2019-05-18 18:50:28.462018\n",
      "Creating the model is completed : 2019-05-18 18:51:49.180635\n"
     ]
    }
   ],
   "source": [
    "highlowrated_sentence=create_sentence( Dataset['Hotel'].isin(list(city_review[(city_review['overall_ratingsource']<=2) | (city_review['overall_ratingsource']>=4.7)]['doc_id'])) )\n",
    "dictionary = corpora.Dictionary(highlowrated_sentence)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in highlowrated_sentence]\n",
    "print('Creating the model is starting : '+str(datetime.datetime.now()))\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=4, id2word = dictionary, #passes=50, \n",
    "                                       random_state=100,\n",
    "                                       update_every=1,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       alpha='auto',\n",
    "                                       per_word_topics=True)\n",
    "\n",
    "print('Creating the model is completed : '+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common words\n",
    "Dataset[\"NormalisedContent\"] = Dataset[\"Content\"].apply(normalise)\n",
    "full_vocab = defaultdict(int)\n",
    "i=0\n",
    "for review in Dataset[\"NormalisedContent\"] :\n",
    "    i=i+1\n",
    "    if review!=\"\":\n",
    "        for token in review.split():\n",
    "            full_vocab[token] += 1\n",
    "\n",
    "list_of_impo_words=[]\n",
    "full_vocab_c=Counter(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_impo_words=[]\n",
    "for (w,s) in full_vocab_c.most_common(50):\n",
    "    if w not in Stopwords :\n",
    "        list_of_impo_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "similiars=load_model_file(\"delhi\", list_of_impo_words, 1)\n",
    "\n",
    "london_similiar=load_model_file(\"london\", list_of_impo_words, 1)\n",
    "similiars=pd.concat([similiars,london_similiar],ignore_index=True,axis=0, sort=True)\n",
    "\n",
    "newyork_similiar=load_model_file(\"newyork\", list_of_impo_words, 1)\n",
    "similiars=pd.concat([similiars,newyork_similiar],ignore_index=True,axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-analyzed topic \n",
    "\n",
    "-   Topic 0 : Sight seeing    \n",
    "-   Topic 1 : Hostel    \n",
    "-   Topic 2 : Food    \n",
    "-   Topic 3 : Staff Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_node_words=list(set(list(similiars[\"s_word\"])+list_of_impo_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "impo_wtopic_id=[]\n",
    "impo_wtopic_name=[]\n",
    "for wtopic in All_node_words:\n",
    "    try :\n",
    "        topic_id=ldamodel[dictionary.doc2bow([wtopic])][1][0][1][0]\n",
    "        if topic_id==0 :\n",
    "            topic_name=\"Sight seeing\"\n",
    "        elif topic_id==1 :\n",
    "            topic_name=\"Hostel\"\n",
    "        elif topic_id==2 :\n",
    "            topic_name=\"Food\"\n",
    "        elif topic_id==3 :\n",
    "            topic_name=\"Staff Service\"\n",
    "        \n",
    "        impo_wtopic_id.append(topic_id)\n",
    "        impo_wtopic_name.append(topic_name)\n",
    "    except :\n",
    "        impo_wtopic_id.append(1)\n",
    "        impo_wtopic_name.append(\"Hostel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "node=pd.DataFrame({ \"id\":All_node_words,\"Label\":All_node_words, \"topic_name\":impo_wtopic_name})\n",
    "node.to_csv(\"node_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_df=pd.DataFrame({ \"word\":All_node_words,\"topic_id\":impo_wtopic_id,  \"topic_name\":impo_wtopic_name})\n",
    "similiar_topic=similiars.merge(Topic_df, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=\"delhi_sg\"\n",
    "Edge=pd.DataFrame({\"Source\":similiar_topic[similiar_topic[\"Type\"]==city][\"word\"],\"Target\":similiar_topic[similiar_topic[\"Type\"]==city][\"s_word\"],\"Type\":\"Undirected\", \"Weight\" :similiar_topic[similiar_topic[\"Type\"]==city][\"s_word_rate\"],\"Average Degree\":\"1\"})\n",
    "Edge.to_csv(\"Edge_\"+city+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
